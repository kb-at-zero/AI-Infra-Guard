info:
  name: vllm
  cve: CVE-2025-61620
  summary: Resource exhaustion (DoS) via malicious Jinja template in vLLM's OpenAI-compatible server
  details: |
    A denial-of-service vulnerability exists in vLLM's OpenAI-compatible server due to unsafe handling of user-provided Jinja templates through the `chat_template` and `chat_template_kwargs` parameters. These parameters are used to format conversation history for chat models, but because Jinja supports complex constructs like loops and recursion, attackers can craft templates that exhaust CPU or memory resources.
    The vulnerability arises because the server allows untrusted clients to influence template rendering. Even if `chat_template` is restricted, an attacker can inject a `chat_template` key into `chat_template_kwargs`, which overwrites the intended template during dictionary merging (`dict.update`). This leads to arbitrary template execution and potential service disruption.
    Endpoints accepting these parameters without proper sanitization are vulnerable to resource exhaustion attacks, causing the server to become unresponsive.
  cvss: CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H
  severity: HIGH
  security_advise: |
    1. Upgrade to vLLM version 0.11.0 or later where this issue has been patched.
    2. Restrict access to endpoints allowing `chat_template` and `chat_template_kwargs` to trusted users only.
    3. Sanitize or strictly validate all user inputs before passing them to template rendering functions.
    4. Monitor resource usage on vLLM servers for signs of abnormal consumption indicative of DoS attempts.
rule: version >= "0.5.1" && version < "0.11.0"
references:
  - https://github.com/vllm-project/vllm/security/advisories/GHSA-6fvq-23cw-5628
  - https://github.com/vllm-project/vllm/pull/25794
  - https://github.com/vllm-project/vllm/commit/7977e5027c2250a4abc1f474c5619c40b4e5682f
  - https://github.com/vllm-project/vllm