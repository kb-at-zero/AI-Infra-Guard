info:
  name: vllm
  cve: CVE-2025-61620
  summary: 通过恶意 Jinja 模板在 vLLM 的 OpenAI 兼容服务器中引发资源耗尽（DoS）
  details: |
    在 vLLM 的 OpenAI 兼容服务器中存在一个拒绝服务漏洞，该漏洞是由于不安全地处理用户提供的 Jinja 模板所导致的。这些模板可通过 `chat_template` 和 `chat_template_kwargs` 参数传入，用于格式化聊天模型的对话历史记录。然而，由于 Jinja 支持循环和递归等复杂结构，攻击者可以构造特定模板以消耗大量 CPU 或内存资源。
    此漏洞产生的根本原因在于服务器允许不受信任的客户端影响模板渲染过程。即使对 `chat_template` 进行了限制，攻击者仍可以通过向 `chat_template_kwargs` 中注入 `chat_template` 键，在字典合并过程中（使用 `dict.update`）覆盖原有模板，从而实现任意模板执行并可能导致服务中断。
    若相关接口未对这些参数进行适当清理，则容易受到资源耗尽型攻击，进而导致服务器无响应。
  cvss: CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H
  severity: HIGH
  security_advise: |
    1. 升级至 vLLM 版本 0.11.0 或更高版本，此问题已在该版本中修复。
    2. 将支持 `chat_template` 和 `chat_template_kwargs` 参数的接口访问权限仅限于受信任用户。
    3. 在将用户输入传递给模板渲染函数之前，务必对其进行严格验证或清理。
    4. 监控 vLLM 服务器上的资源使用情况，识别可能表明遭受 DoS 攻击的异常资源消耗行为。
rule: version >= "0.5.1" && version < "0.11.0"
references:
  - https://github.com/vllm-project/vllm/security/advisories/GHSA-6fvq-23cw-5628
  - https://github.com/vllm-project/vllm/pull/25794
  - https://github.com/vllm-project/vllm/commit/7977e5027c2250a4abc1f474c5619c40b4e5682f
  - https://github.com/vllm-project/vllm